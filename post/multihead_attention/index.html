<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Multi-head Attention - Yuko's Note</title>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NC0VXE56DE"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NC0VXE56DE")}</script><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="Yuko Hu"><meta name=description content="詳解 Multi-head Attention 實作。
"><meta name=keywords content="Yuko,yuko29"><meta name=generator content="Hugo 0.129.0 with theme even"><link rel=canonical href=http://yuko29.github.io/post/multihead_attention/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+TC:wght@100..900&family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel=stylesheet><link href=/sass/main.min.65ba10c33328e29aa9919dd2d128178677d70bcc8e284e9829109ce84f8bb988.css rel=stylesheet><meta property="og:url" content="http://yuko29.github.io/post/multihead_attention/"><meta property="og:site_name" content="Yuko's Note"><meta property="og:title" content="Multi-head Attention"><meta property="og:description" content="詳解 Multi-head Attention 實作。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-03-22T13:15:51+08:00"><meta property="article:modified_time" content="2024-03-22T13:15:51+08:00"><meta itemprop=name content="Multi-head Attention"><meta itemprop=description content="詳解 Multi-head Attention 實作。"><meta itemprop=datePublished content="2024-03-22T13:15:51+08:00"><meta itemprop=dateModified content="2024-03-22T13:15:51+08:00"><meta itemprop=wordCount content="1296"><meta itemprop=keywords content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="Multi-head Attention"><meta name=twitter:description content="詳解 Multi-head Attention 實作。"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Yuko's Note</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/about><li class=mobile-menu-item>About</li></a><a href=/categories/><li class=mobile-menu-item>Categories</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Yuko's Note</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/about>About</a></li><li class=menu-item><a class=menu-item-link href=/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Multi-head Attention</h1><div class=post-meta><span class=post-time>2024-03-22</span><div class=post-category><a href=/categories/deep-learning/>Deep Learning</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><a href=#attention>Attention</a></li><li><a href=#multi-head-attention>Multi-head Attention</a><ul><li><a href=#step-1>Step 1</a></li><li><a href=#step-2>Step 2</a></li><li><a href=#step-3>Step 3</a></li><li><a href=#step-4>Step 4</a></li><li><a href=#step-5>Step 5</a></li></ul></li></ul></li></ul></nav></div></div><div class=post-content><p>詳解 Multi-head Attention 實作。</p><h2 id=attention>Attention</h2><p>Attention 的算法公式列在下方：</p>$$A(Q, K, V) = \text{softmax} ( \frac{QK^T}{\sqrt{d_k}})V$$<p>其中，Q, K, V 矩陣是 input tensor 經過分別三個 linear projection 得到。<br>上述的 attention 是 single head 的，只有計算一組 Q, K, V。所謂 Multi-head Attention，則是希望同時有多組的 Q, K, V，讓模型可以學習到不同的 feature。</p><h2 id=multi-head-attention>Multi-head Attention</h2><p>先上程式碼，以 Huggingface 的 LlamaAttention 實作舉例，只留下核心算法的部份：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>LlamaAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Multi-headed attention from &#39;Attention Is All You Need&#39; paper&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>LlamaConfig</span><span class=p>,</span> <span class=n>ratio</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>num_attention_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>max_position_embeddings</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>max_position_embeddings</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ratio</span> <span class=o>=</span> <span class=n>ratio</span> <span class=c1># 1 means no truncate, just keep normal attn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>)</span> <span class=o>!=</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=sa>f</span><span class=s2>&#34;hidden_size must be divisible by num_heads (got `hidden_size`: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>                <span class=sa>f</span><span class=s2>&#34; and `num_heads`: </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=si>}</span><span class=s2>).&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>o_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rotary_emb</span> <span class=o>=</span> <span class=n>LlamaRotaryEmbedding</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>,</span> <span class=n>max_position_embeddings</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>max_position_embeddings</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>hidden_states</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>position_ids</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>past_key_value</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>output_attentions</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>use_cache</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>],</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]]:</span>
</span></span><span class=line><span class=cl>            <span class=n>bsz</span><span class=p>,</span> <span class=n>q_len</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>hidden_states</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>query_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>q_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>key_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>q_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>q_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>query_states</span><span class=o>.</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>value_states</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=c1># group by token</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>q_len</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>以下分段解析 forward 的部份。</p><h3 id=step-1>Step 1</h3><p>在實作方法中，Q, K, V 的 linear projection 仍然只用各一個 linear projection 完成 (<code>q_proj</code>, <code>k_proj</code>, <code>v_proj</code>)。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>query_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>q_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>key_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>q_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>bsz</span><span class=p>,</span> <span class=n>q_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><img src=/post/multihead_attention/images/inputq.svg style=display:block;margin:auto>
<img src=/post/multihead_attention/images/qko.svg style=display:block;margin:auto><h3 id=step-2>Step 2</h3><div class="admonition info"><p class=admonition-title>info</p><p>先提一下整個 MHA 實作的中心思想：<br>把原本 hidden dim 平均拆成 num_heads 塊，這些子塊代表不同的 head，各自獨立計算 attention，最後再將結果拼回一起。</p></div><p>此步驟的目的是要將 Q, K, V 沿著 hidden dim 維度平均拆成 num_heads 份。</p><p>對 Q, K, V 做 reshape，把原本 token 的 hidden dim 切成 num_heads x head_dim，如下方中間的圖。每個 token 的 hidden dim 被分成 num_heads 組了。
再來，做 transpose 把 num_heads 和 seq_len 這兩個維度對調，把同的 head 部份拼在一起。</p><img src=/post/multihead_attention/images/reshape_transpose.svg style=display:block;margin:auto><p>到這邊可以發現，這兩個步驟其實等同於把原本的矩陣依照 hidden dim 維度分割成 num_heads 塊。<br>因此我們得到了 Multi-head Attention 需要的多組 Q, K, V (即 \(q_i, k_i, v_i, i=0, 1, \dots \text{,num_heads-1}\))。</p><img src=/post/multihead_attention/images/qko_transpose.svg style=display:block;margin:auto><p>好像有點難想像？</p><p>舉間單例子：seq_len = 4, hidden_dim = 6, num_heads = 2, head_dim = 3</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>&gt;&gt;&gt; a = torch.rand(4,6)
</span></span><span class=line><span class=cl>&gt;&gt;&gt; a
</span></span><span class=line><span class=cl>tensor([[0.7924, 0.4975, 0.5119, 0.4034, 0.6843, 0.3314],
</span></span><span class=line><span class=cl>        [0.9629, 0.7819, 0.5565, 0.5319, 0.2773, 0.2841],
</span></span><span class=line><span class=cl>        [0.3263, 0.7731, 0.5472, 0.6618, 0.3387, 0.6278],
</span></span><span class=line><span class=cl>        [0.7786, 0.0196, 0.0878, 0.0646, 0.6827, 0.6362]])
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt;&gt;&gt; a.view(4,2,3)
</span></span><span class=line><span class=cl>tensor([[[0.7924, 0.4975, 0.5119],
</span></span><span class=line><span class=cl>         [0.4034, 0.6843, 0.3314]],
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        [[0.9629, 0.7819, 0.5565],
</span></span><span class=line><span class=cl>         [0.5319, 0.2773, 0.2841]],
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        [[0.3263, 0.7731, 0.5472],
</span></span><span class=line><span class=cl>         [0.6618, 0.3387, 0.6278]],
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        [[0.7786, 0.0196, 0.0878],
</span></span><span class=line><span class=cl>         [0.0646, 0.6827, 0.6362]]])
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>&gt;&gt;&gt; a.view(4,2,3).transpose(0,1)
</span></span><span class=line><span class=cl>tensor([[[0.7924, 0.4975, 0.5119],
</span></span><span class=line><span class=cl>         [0.9629, 0.7819, 0.5565],
</span></span><span class=line><span class=cl>         [0.3263, 0.7731, 0.5472],
</span></span><span class=line><span class=cl>         [0.7786, 0.0196, 0.0878]],
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>          
</span></span><span class=line><span class=cl>        [[0.4034, 0.6843, 0.3314],
</span></span><span class=line><span class=cl>         [0.5319, 0.2773, 0.2841],
</span></span><span class=line><span class=cl>         [0.6618, 0.3387, 0.6278],
</span></span><span class=line><span class=cl>         [0.0646, 0.6827, 0.6362]]])
</span></span></code></pre></td></tr></table></div></div><p>很巧妙對吧。看到這裡我不禁驚嘆出聲。</p><h3 id=step-3>Step 3</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>attn_weights</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>query_states</span><span class=o>.</span><span class=n>dtype</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>attn_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>value_states</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>對各組 \(q_i, k_i, v_i\) 個別計算 attention，得到單個 head 的 attention output \(s_i\)，這邊利用高維矩陣相乘實作。<br>高維矩陣相乘，其實就是對於高維矩陣中的每個二維矩陣做矩陣乘法。剛剛處理完的 Q, K, V 裡含有 num_heads 個二維矩陣，因此對對應的 \(q_i,k_i^T,v_i\) 計算 attention 這件事在編寫程式上就可以用兩次矩陣乘法完成。</p><img src=/post/multihead_attention/images/mha.svg style=display:block;margin:auto><h3 id=step-4>Step 4</h3><p>計算完所有 heads 的 attention 得到 num_heads 個 \(s_i\) 後，實行 <a href=#step-2>Step 2</a> 的逆操作。<br>先將 seq_len 和 num_heads 維度對調 (transpose) ，相當於把屬於個別 token 的 hidden dim 子塊拼起來，再把矩陣 reshape 成 (seq_len, hidden dim)。<br>至此，原本各自計算的 \(s_i\) 被合回一個二維矩陣，完成 Multi-head Attention 的計算。</p><img src=/post/multihead_attention/images/transpose_back.svg style=display:block;margin:auto><h3 id=step-5>Step 5</h3><p>在 Multi-head Attention 運算最後，常會把結果再做一次 linear projection，稱為 output projection。得到的結果為最終 Multi-head Attention 的 output。</p><img src=/post/multihead_attention/images/output.svg style=display:block;margin:auto><blockquote><p><strong>後記</strong></p><p>看了很多 Multihead attention 的解說但一直沒能懂，那些圖完全沒能輔助我解讀計算過程。徹底把程式碼拆出來看後我才徹徹底底的理解 MHA 到底是個什麼樣的機制，並把理論很緊密和實作的關聯起來。因此決定寫一篇 blog 把我的理解過程寫下來備忘，並順手畫一些圖輔助理解。</p></blockquote><a class=hugo-shortcodes-bmc-button href=https://www.buymeacoffee.com/yuko29><img src="https://img.buymeacoffee.com/button-api/?button_colour=ffdd00&amp;coffee_colour=ffffff&amp;emoji=&amp;font_colour=000000&amp;font_family=Comic&amp;outline_colour=000000&amp;slug=yuko29&amp;text=Buy+me+a+coffee" alt="Buy me a coffee">
</a><iframe class=LikeCoin height=235 src="https://button.like.co/in/embed/yuko2926846/button?referrer=http%3a%2f%2fyuko29.github.io%2fpost%2fmultihead_attention%2f" width=100% frameborder=0></iframe></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Yuko Hu</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2024-03-22</span></p><p class=copyright-item><span class=item-title>License</span>
<span class=item-content><a rel="license noopener" href=https://creativecommons.org/licenses/by-sa/4.0/deed.zh-hant target=_blank>CC BY-SA 4.0</a></span></p></div><footer class=post-footer><nav class=post-nav><a class=next href=/post/llm_evaluation/><span class="next-text nav-default">LLM Evaluation</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname==="localhost")return;var t,e=document.createElement("script");e.type="text/javascript",e.async=!0,t="yuko29",e.src="//"+t+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></main><footer id=footer class=footer><div class=social-links><a href=mailto:yuko29.hu@email.com class="iconfont icon-email" title=email></a><a href=http://github.com/yuko29 class="iconfont icon-github" title=github></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span><span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span><span class=copyright-year>&copy;
2020 -
2025<span class=heart><i class="iconfont icon-heart"></i></span><span>Yuko Hu</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js></script><script type=text/javascript>window.MathJax={tex:{}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NC0VXE56DE"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NC0VXE56DE")}</script></body></html>